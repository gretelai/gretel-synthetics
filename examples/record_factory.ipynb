{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# This Notebook serves as RFC / demo for a new low level API for synthetic record generation. Currently, when training\n",
    "# on a DataFrame, we will break the columns up into clusters and train individual models on DataFrames built\n",
    "# from those clusters. So a 3-cluster model will have 3 actual TF models where each TF model covers a certain subset\n",
    "# of columns.\n",
    "#\n",
    "# When we generate data, let's say 500 records, we generate the 500 records for each batch, buffering them into\n",
    "# memory and re-creating DataFrames and eventuall concat these DFs together. This uses a growing unbounded amount of\n",
    "# memory and also makes it challenging to do entire record validation.\n",
    "#\n",
    "# This update introduces a new low-level primitive, ``RecordFactory`` that uses an underlying generator to load all\n",
    "# the TF models into memory initially, then generate partial records sequentially for each model and construct an\n",
    "# entire record at a time.\n",
    "#\n",
    "# For this demo, we'll use an already built model, which you can download and extract to it's own directory, for this\n",
    "# demo I use \"test-model\" as the checkpoint dir.\n",
    "#\n",
    "# https://gretel-public-website.s3-us-west-2.amazonaws.com/tests/synthetics/models/safecast-batch-sp-0-14.tar.gz\n",
    "#\n",
    "from gretel_synthetics.batch import DataFrameBatch\n",
    "\n",
    "batch = DataFrameBatch(mode=\"read\", checkpoint_dir=\"test-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create the factory with a method on the batch object, you can also provide the entire record\n",
    "# validator directly to the method.\n",
    "\n",
    "def validator(rec: dict):\n",
    "    \"\"\"NOTE: The values of each record will be the raw strings\n",
    "    that were generated from the NN so you will have to handle\n",
    "    any type casting.\n",
    "    \"\"\"\n",
    "    assert float(rec[\"payload.loc_lat\"])\n",
    "    \n",
    "\n",
    "factory = batch.create_record_factory(num_lines=50, validator=validator)\n",
    "type(factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The factory is stateful, and is designed to track its current capacity to generate\n",
    "factory.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# The entire factory can be treated as an iterator, and it will only provide valid records while still tracking\n",
    "# the number of invalid records under the hood (NOTE: This model might need a few iterations to generate invalids)\n",
    "rec = next(factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that we have incremented our valid count now\n",
    "factory.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-legislature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can exhaust the rest of the underlying record iterator\n",
    "the_rest = list(factory)\n",
    "the_rest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(the_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe we also experienced some invalid records while generating\n",
    "factory.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The factory is now exhaused:\n",
    "assert list(factory) == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can reset it's state so we're ready to generate again:\n",
    "factory.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll update the validator with a validator that always fails and update our ``max_invalid`` for demo purposes\n",
    "# This will force the RunTimeError when generating\n",
    "factory.validator = lambda x: False\n",
    "factory.max_invalid = 10\n",
    "list(factory)\n",
    "\n",
    "# invalid count should match the max invalid now\n",
    "print(factory.summary)\n",
    "factory.validator = validator  # reset our original validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's another utility that will auto-reset the factory state and attempt to generate all records. Optionally\n",
    "# with a specific output type. Currently DFs are supported.\n",
    "#\n",
    "# This will buffer records as they are generated, so will consume memory in that sense. When returning a DF, we'll \n",
    "# try to infer the dtypes as if we are loading the DF from a CSV on disk.\n",
    "\n",
    "syn_df = factory.generate_all(output=\"df\")\n",
    "print(syn_df.head())\n",
    "syn_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next steps\n",
    "# - Do we still need a progress bar chart to show the number of invalid records? That has confused folks previously.\n",
    "# - Create some other helper methods directly on the DataFrameBatch to get out a synthetic DF using this factory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
