{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gretel Synthetics Walkthrough\n",
    "\n",
    "Welcome to the Gretel Synthetics walkthrough! In this tutorial we will take you through the steps of extracting data from Gretel, building a training dataset, creating synthetic data, and validating the new data!\n",
    "\n",
    "This tutorial assumes you have already created and uploaded data to a [Gretel project](https://console.gretel.cloud).\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "- If using Google Colab, we recommend you change to a GPU runtime. From the menu, choose \"Runtime\" and then choose \"Change runtime type\"\n",
    "\n",
    "- Input your Gretel URI String. Just run the cell below (no need to change it's contents) and then enter your Gretel URI in the pop-up box when it appears. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "gretel_uri = os.getenv(\"GRETEL_URI\") or getpass.getpass(\"Your Gretel URI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to create a synthetic dataset\n",
    "\n",
    "\n",
    "In the code below, we will:\n",
    "* Install Gretel packages and dependencies\n",
    "* Optionally connect to Gretel API and download source data the project stream\n",
    "* Automatically build a record validator from the source data\n",
    "* Train a synthetic model (neural network) on the source data\n",
    "* Generate `gen_lines` synthetic data records that pass validation\n",
    "* Create a synthetic data performance report to compare the source and synthetic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "gretel_synthetics_deps",
    "cell_replaced": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -U gretel-client\n",
    "\n",
    "# NOTE: if you need synthetics, but already have TensorFlow installed (like in Colab) install below\n",
    "!pip install gretel-synthetics\n",
    "\n",
    "# NOTE: if you need synthetics AND TensorFlow, use the below\n",
    "# !pip install gretel-synthetics[tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "gretel_synthetics_boilerplate",
    "cell_replaced": true
   },
   "outputs": [],
   "source": [
    "from gretel_client import project_from_uri\n",
    "\n",
    "project = project_from_uri(gretel_uri)\n",
    "project.client.install_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training DataFrame\n",
    "\n",
    "Here you have one of three options:\n",
    "\n",
    "1) Download records from Gretel (using your Gretel URI from before)\n",
    "2) You can provide an absolute path to your own CSV\n",
    "3) You can create your own code to generate your own DataFrame however you like\n",
    "\n",
    "By default we suggest filtering fields based on percent unique and percent missing. We reccomend using fields that have no more than 80% uniqueness and are missing no more than 20% of the time. Feel free to adjust these parameters.\n",
    "\n",
    "If you wish to use all fields, you can omit the returned ``include_fields`` list from the synthetic bundle creation below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_helpers.synthetics import create_df, SyntheticDataBundle\n",
    "\n",
    "# NOTE: You can change the first argument to a CSV file of your choice\n",
    "# to load your own data outside of a Gretel Project. If you want to use the\n",
    "# entire CSV in this case, set ``num_rows`` to ``None.``\n",
    "\n",
    "training_df = create_df(\n",
    "    gretel_uri,  # This can be changed to a CSV path (local Filesystem, S3, etc)\n",
    "    num_rows=5000,  # set to ``None`` to include all records\n",
    "    max_unique_percent=80,  # set to 100 to include all columns\n",
    "    max_missing_percent=20  # set to 100 to include all columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have the DataFrame that will be used for training, this can be manipulated beforehand\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Gretel Synthetic Bundle\n",
    "\n",
    "Next, we run our bundle automation process. This automates the following actions:\n",
    "\n",
    "- Automatically detect a field delimiter to be used for the Gretel Synthetics library\n",
    "- Automatically detect correlations between columns and create batches of column headers for synthesis\n",
    "- Build data validators that ensure generated records are within a range of boundaries learned from your training data\n",
    "- Build neural network models\n",
    "- Utilize AI models to create synthetic data\n",
    "\n",
    "\n",
    "# Synthetic Configuration\n",
    "\n",
    "- See [our documentation](https://gretel-synthetics.readthedocs.io/en/stable/api/config.html) for additional config options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Gretel Synthtetics Training / Model Configuration\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_dir = str(Path.cwd() / \"checkpoints\")\n",
    "\n",
    "config_template = {\n",
    "    \"checkpoint_dir\": checkpoint_dir,\n",
    "    \"dp\": True, # enable differential privacy in training\n",
    "    \"epochs\": 15,\n",
    "    \"gen_lines\": 100,\n",
    "    \"overwrite\": True,\n",
    "    \"save_all_checkpoints\": False,\n",
    "    \"vocab_size\": 20000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = SyntheticDataBundle(\n",
    "    training_df=training_df,\n",
    "    delimiter=None, # if ``None``, it will try and automatically be detected, otherwise you can set it\n",
    "    auto_validate=True, # build record validators that learn per-column, these are used to ensure generated records have the same composition as the original\n",
    "    synthetic_config=config_template, # the config for Synthetics\n",
    "    # sample_cutoff=100000, # if the training DF has more rows than this, we will use this value to sample records for delim detection, header clustering, and data validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional params:\n",
    "#\n",
    "# num_lines=500 will override the synthetic config ``num_lines``, set whatever number you need\n",
    "# max_invalid=5000 will override the default invalid line limit that terminates execution, set whatever number you need\n",
    "\n",
    "bundle.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle.get_synthetic_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Report\n",
    "\n",
    "The Performance Report compares the training data to the newly created synthetic data and assesses their statistical similarity.   It shows you both quantitatively and graphically any differences between within field distributions as well as cross field correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle.generate_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
