{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standlone Gretel tokenizers wrap any tokenizer in a simple abstract API, these tokenizers can be used\n",
    "# during training / generation. Below is an example of using two built-in tokenizers we have on some training\n",
    "# data. Aside from the classes used, the API is the same, which makes swapping out tokenizers trivial.\n",
    "\n",
    "# There are separate clases for training and loading a tokenizer. This is because when loading a tokenizer model\n",
    "# back in, we aren't really loading the training params that were used for them.\n",
    "\n",
    "from gretel_synthetics.base_config import BaseConfig\n",
    "\n",
    "# We'll make a base config that doesn't actually train a model, it just creates\n",
    "# our shell model directory, we'll just store the tokenizer data here\n",
    "config = BaseConfig(input_data_path=\"poe.txt\", checkpoint_dir=\"tokenizer_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the basic char/idx mapping tokenizer\n",
    "\n",
    "from gretel_synthetics.tokenizers.char import CharTokenizerTrainer, CharTokenizer\n",
    "\n",
    "trainer = CharTokenizerTrainer(config=config)\n",
    "\n",
    "# First we build our annotated trainig data, for char tokenizer, it just makes a copy of the training\n",
    "# data. This writes the training data to our model directory\n",
    "trainer.create_annotated_training_data()\n",
    "\n",
    "# This builds the char/idx mappings and saves the model to disk\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load our tokenizer back in from the saved model\n",
    "\n",
    "tok = CharTokenizer.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tokenizer has basic API interfaces\n",
    "print(tok.total_vocab_size)\n",
    "ids = tok.encode_to_ids(\"Once upon a midnight dreary, while I pondered, weak and weary,\")\n",
    "print(ids)\n",
    "\n",
    "# now back to the string\n",
    "print(tok.decode_from_ids(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same exact interface methods, but now using SentencePiece as the underlying tokenizer\n",
    "\n",
    "from gretel_synthetics.tokenizers.sentencepiece import SentencepieceTokenizerTrainer, SentencePieceTokenizer\n",
    "\n",
    "trainer = SentencepieceTokenizerTrainer(config=config)\n",
    "\n",
    "trainer.create_annotated_training_data()\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the same interface for SP as the simple Char tokenizer\n",
    "tok = SentencePieceTokenizer.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our tokenizer has basic API interfaces\n",
    "print(tok.total_vocab_size)\n",
    "ids = tok.encode_to_ids(\"Once upon a midnight dreary, while I pondered, weak and weary,<n>\")\n",
    "print(ids)\n",
    "\n",
    "# now back to the string\n",
    "print(tok.decode_from_ids(ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
